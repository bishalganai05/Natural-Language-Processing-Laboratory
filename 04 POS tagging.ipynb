{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"id":"669z9e02hkUA","executionInfo":{"status":"ok","timestamp":1701268850588,"user_tz":-330,"elapsed":1962,"user":{"displayName":"Bishal Ganai","userId":"14338647434661618710"}},"outputId":"78f0422d-a164-4b6e-8d59-08558673cbb2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqC7l46OWnIF","outputId":"f937d00f-dd4c-4ac7-c94d-6a188b180d45","executionInfo":{"status":"ok","timestamp":1701268858497,"user_tz":-330,"elapsed":647,"user":{"displayName":"Bishal Ganai","userId":"14338647434661618710"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package state_union to /root/nltk_data...\n","[nltk_data]   Package state_union is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["['Good morning everyone.', 'Today we will study NLTK.']\n","[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n","[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('study', 'VB'), ('NLTK', 'NNP'), ('.', '.')]\n"]}],"source":["import nltk\n","from nltk.corpus import state_union\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"state_union\")\n","from nltk.tokenize.punkt import PunktSentenceTokenizer\n","tokenizer = PunktSentenceTokenizer()\n","text = \"Good morning everyone. Today we will study NLTK.\"\n","custom_sent_tokenizer = PunktSentenceTokenizer(text)\n","tokenized = custom_sent_tokenizer.tokenize(text)\n","print(tokenized)\n","\n","\n","def process_content():\n","    try:\n","        for i in tokenized:\n","            words = nltk.word_tokenize(i)\n","            tagged = nltk.pos_tag(words)\n","            print(tagged)\n","    except Exception as e:\n","        print(str(e))\n","process_content()"]}]}